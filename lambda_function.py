import json
import os
import openai

openai.api_key = os.environ['OPENAI_API_KEY']

def lambda_handler(event, context):
    body = json.loads(event.get('body','{}'))
    user_message = body.get('message', '')

    # If message == "INIT", simulate initial response
    if user_message == "INIT":
        # Start by asking name
        return success_response("Hello!\n\nPlease type your name to begin:\n[ASK_NAME]")

    # This logic should match what you had in app.py
    # Due to complexity, you can replicate similar logic here
    # For brevity, we just return a static response.

    # In real scenario, you'd implement the same conversation logic here as in your Flask code.
    # The difference: no Flask. Just handle user_message and return JSON.
    # Example:
    if "name" not in context.client_context or not context.client_context['name']:
        # If no name known, guess user is at ask_name stage
        if user_message:
            # store name somehow or pass it in next calls
            return success_response(f"Nice to meet you, {user_message}!\n\nPlease select an option above.\n[MAIN_MENU]")
        else:
            return success_response("Please type your name:\n[ASK_NAME]")

    # Add your conversation logic from the Flask code here...
    # For demonstration, return a fallback message:
    return success_response("Iâ€™m not fully implemented yet.\n[MAIN_MENU]")

def success_response(msg):
    return {
        "statusCode": 200,
        "headers": {"Content-Type":"application/json"},
        "body": json.dumps({"response": msg})
    }
